{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MLP for FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a progress bar for training later, install tqdm and ipywidgets.\n",
    "If you use conda: conda install tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the dataset comfortably using the torchvision library.\n",
    "Data instances correspond to images of fashion articles and are labelled as one of 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_set_full = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcac933d3a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, target = train_set_full[231]\n",
    "plt.imshow(img.view(28, 28), cmap=\"binary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since FashionMNIST only gives us a train and a test set, we split the test set further into a train and a validation set.\n",
    "\n",
    "Q: What would happen if we used the test set instead of a validation set to tune our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "train_indices, val_indices = sklearn.model_selection.train_test_split(\n",
    "    range(len(train_set_full)),\n",
    "    stratify=train_set_full.targets,\n",
    "    test_size=val_size,\n",
    "    random_state=0,\n",
    ")\n",
    "train_set = torch.utils.data.Subset(train_set_full, train_indices)\n",
    "val_set = torch.utils.data.Subset(train_set_full, val_indices)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=500, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=500, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=500, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a feedforward neural network that can process 28 by 28 images (e.g., by flattening the image into a single vector).\n",
    "\n",
    "You can look up network components in the pytorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, hidden_dim, out_dim=10, img_shape=(28, 28),) -> None:\n",
    "        super().__init__()\n",
    "        in_dim = img_shape[0] * img_shape[1]\n",
    "        self.img_shape = img_shape\n",
    "        self.lin1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x has shape (batch_size, *img_size)\"\"\"\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        logits = self.lin2(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following functions that will be used in the overall training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> float:\n",
    "    \"\"\"Train a model for one epoch\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): model to be trained\n",
    "        loader (torch.utils.data.DataLoader): Dataloader for training data\n",
    "        criterion (torch.nn.Module): loss function\n",
    "        optimizer (torch.optim.Optimizer): optimizer\n",
    "\n",
    "    Returns:\n",
    "        float: total loss over one epoch\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)  # I am using device as a global variable, but you could pass it as well\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()  # we dont want these operations to be recorded for automatic differentation, saves memory\n",
    "def validate(\n",
    "    model: torch.nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module = None,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Compute total loss and accuracy\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): model to be evaluated\n",
    "        loader (torch.utils.data.DataLoader): Dataloader for evaluation data\n",
    "        criterion (torch.nn.Module, optional): loss function. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: total loss, accuracy\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        if criterion is not None:\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "        total_correct += (out.argmax(dim=1) == y).sum().item()\n",
    "    return total_loss, total_correct / len(loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What accuracy can you reach on the validation set when trying out different model sizes and learning rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_loss=73.922, train_acc=0.725, val_loss=18.770, val_acc=0.722\n",
      "epoch=1: train_loss=60.798, train_acc=0.783, val_loss=15.457, val_acc=0.774\n",
      "epoch=2: train_loss=53.770, train_acc=0.810, val_loss=13.677, val_acc=0.803\n",
      "epoch=3: train_loss=51.106, train_acc=0.815, val_loss=12.951, val_acc=0.806\n",
      "epoch=4: train_loss=49.621, train_acc=0.822, val_loss=12.534, val_acc=0.810\n",
      "epoch=5: train_loss=46.328, train_acc=0.832, val_loss=11.807, val_acc=0.824\n",
      "epoch=6: train_loss=45.944, train_acc=0.836, val_loss=11.645, val_acc=0.827\n",
      "epoch=7: train_loss=47.443, train_acc=0.832, val_loss=12.077, val_acc=0.822\n",
      "epoch=8: train_loss=43.161, train_acc=0.842, val_loss=10.964, val_acc=0.835\n",
      "epoch=9: train_loss=42.312, train_acc=0.848, val_loss=10.862, val_acc=0.840\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 200\n",
    "learning_rate = 1e-1\n",
    "\n",
    "model = FeedForwardNet(hidden_dim=hidden_dim).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 10  # change this as needed\n",
    "for epoch in range(n_epochs):\n",
    "    train_epoch(model, train_loader, criterion, optimizer)\n",
    "    train_loss, train_acc = validate(model, train_loader, criterion=criterion)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion=criterion)\n",
    "    print(\n",
    "        f\"{epoch=}: {train_loss=:.3f}, {train_acc=:.3f}, {val_loss=:.3f}, {val_acc=:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b31a3aaeeda4af9c04893663190159bf2000bd9935849b890345d37678b4dc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('idl21': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
