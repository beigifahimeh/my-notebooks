{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MLP for FashionMNIST\n",
    "\n",
    "We revisit FashionMNIST to see whether regularization improves our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import sklearn.model_selection\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_set_full = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, target = train_set_full[231]\n",
    "plt.imshow(img.view(28, 28), cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "train_indices, val_indices = sklearn.model_selection.train_test_split(\n",
    "    range(len(train_set_full)),\n",
    "    stratify=train_set_full.targets,\n",
    "    test_size=val_size,\n",
    "    random_state=0,\n",
    ")\n",
    "train_set = torch.utils.data.Subset(train_set_full, train_indices)\n",
    "val_set = torch.utils.data.Subset(train_set_full, val_indices)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=500, shuffle=True, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1000, shuffle=False, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=500, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a feedforward neural network that can process 28 by 28 images (e.g., by flattening the image into a single vector).\n",
    "\n",
    "You can look up network components in the pytorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        out_dim=10,\n",
    "        img_shape=(28, 28),\n",
    "        n_layers: int = 3,\n",
    "        p: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        in_dim = img_shape[0] * img_shape[1]\n",
    "        self.img_shape = img_shape\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Sequential(nn.Linear(in_dim, hidden_dim), nn.ReLU()))\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Dropout(p=p)\n",
    "                )\n",
    "            )\n",
    "        self.layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x has shape (batch_size, *img_size)\"\"\"\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the functionality to use L1 loss in `train_epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    l1_coeff:float=0\n",
    ") -> float:\n",
    "    \"\"\"Train a model for one epoch\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): model to be trained\n",
    "        loader (torch.utils.data.DataLoader): Dataloader for training data\n",
    "        criterion (torch.nn.Module): loss function\n",
    "        optimizer (torch.optim.Optimizer): optimizer\n",
    "        l1_coeff (float): coefficient of L1 loss\n",
    "\n",
    "    Returns:\n",
    "        float: total loss over one epoch\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)  # I am using device as a global variable, but you could pass it as well\n",
    "        out = model(x)\n",
    "        if l1_coeff != 0:\n",
    "            params = torch.concat([params.view(-1) for params in model.parameters()])\n",
    "            l1_loss = F.l1_loss(params, torch.zeros_like(params))\n",
    "            loss = criterion(out, y) + l1_coeff * l1_loss\n",
    "        else:\n",
    "            loss = criterion(out, y)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()  # we dont want these operations to be recorded for automatic differentation, saves memory\n",
    "def validate(\n",
    "    model: torch.nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module = None,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Compute total loss and accuracy\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): model to be evaluated\n",
    "        loader (torch.utils.data.DataLoader): Dataloader for evaluation data\n",
    "        criterion (torch.nn.Module, optional): loss function. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: total loss, accuracy\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        if criterion is not None:\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "        total_correct += (out.argmax(dim=1) == y).sum().item()\n",
    "    return total_loss, total_correct / len(loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with a fixed architecture and add regularization one by one. Keep track of what regularization are added. How does your performance change and what is your optimal setup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 200\n",
    "learning_rate = 1e-1\n",
    "\n",
    "model = FeedForwardNet(hidden_dim=hidden_dim, p=0.5, n_layers=3).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-3, momentum=0.9, nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 50  # change this as needed\n",
    "start = perf_counter()\n",
    "for epoch in range(n_epochs):\n",
    "    train_epoch(model, train_loader, criterion, optimizer, l1_coeff=0)\n",
    "    train_loss, train_acc = validate(model, train_loader, criterion=criterion)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion=criterion)\n",
    "    print(\n",
    "        f\"{perf_counter() - start:.1f}s {epoch=}: {train_loss=:.3f}, {train_acc=:.3f}, {val_loss=:.3f}, {val_acc=:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.048309087753296, 0.8738)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = validate(model, test_loader, criterion=criterion)\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried out several regularization combinations, but with my specific setup I did not see large gains in validation or test accuracy.\n",
    "For example, increasing L2 regularization from 1e-5 to 1e-3 does decrease the gap between training and validation accuracy, but it does not increase test performance.\n",
    "One reason may be that the fully connected network is not particularly suited for this problem.\n",
    "\n",
    "| Layers | Hidden | LR   | Epochs | Optimizer                   | Batch size | Dropout | L1   | L2   | Early Stopping | Train Acc | Val Acc | Test Acc |\n",
    "|--------|--------|------|--------|-----------------------------|------------|---------|------|------|----------------|-----------|---------|----------|\n",
    "| 3      | 200    | 1e-1 | 50     | SGD (Nesterov momentum=0.9) | 500        | 0       | 0    | 0    | No             | 0.971     | 0.888   | 0.8815   |\n",
    "| 3      | 200    | 1e-1 | 50     | SGD (Nesterov momentum=0.9) | 500        | 0.5     | 0    | 0    | No             | 0.953     | 0.891   |          |\n",
    "| 3      | 200    | 1e-1 | 50     | SGD (Nesterov momentum=0.9) | 500        | 0.5     | 0    | 1e-5 | No             | 0.953     | 0.889   |  0.8825  |\n",
    "| 3      | 200    | 1e-1 | 50     | SGD (Nesterov momentum=0.9) | 500        | 0.5     | 0    | 1e-3 | No             | 0.901     | 0.874   |  0.8757  |\n",
    "| 3      | 200    | 1e-1 | 50     | SGD (Nesterov momentum=0.9) | 500        | 0.5     | 1e-5 | 1e-5 | No             | 0.955     | 0.889   | 0.8871   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b31a3aaeeda4af9c04893663190159bf2000bd9935849b890345d37678b4dc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('idl21': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
